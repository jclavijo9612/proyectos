{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0482618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset original: 41188\n",
      "Tamaño del dataset remanente: 4119\n",
      "    age           job  marital            education  default housing loan  \\\n",
      "9    25      services   single          high.school       no     yes   no   \n",
      "55   55    technician  married  professional.course  unknown     yes   no   \n",
      "60   47  entrepreneur  married    university.degree  unknown      no   no   \n",
      "77   33      services  married          high.school  unknown     yes   no   \n",
      "82   38        admin.  married          high.school  unknown      no   no   \n",
      "\n",
      "      contact month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
      "9   telephone   may         mon  ...         1    999         0  nonexistent   \n",
      "55  telephone   may         mon  ...         1    999         0  nonexistent   \n",
      "60  telephone   may         mon  ...         1    999         0  nonexistent   \n",
      "77  telephone   may         mon  ...         2    999         0  nonexistent   \n",
      "82  telephone   may         mon  ...         2    999         0  nonexistent   \n",
      "\n",
      "   emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
      "9           1.1          93.994          -36.4      4.857       5191.0  no  \n",
      "55          1.1          93.994          -36.4      4.857       5191.0  no  \n",
      "60          1.1          93.994          -36.4      4.857       5191.0  no  \n",
      "77          1.1          93.994          -36.4      4.857       5191.0  no  \n",
      "82          1.1          93.994          -36.4      4.857       5191.0  no  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "               age     duration     campaign        pdays     previous  \\\n",
      "count  4119.000000  4119.000000  4119.000000  4119.000000  4119.000000   \n",
      "mean     39.935178   263.372420     2.602573   962.818160     0.167516   \n",
      "std      10.456897   269.761349     2.849061   186.140544     0.500222   \n",
      "min      18.000000     4.000000     1.000000     0.000000     0.000000   \n",
      "25%      32.000000   102.000000     1.000000   999.000000     0.000000   \n",
      "50%      38.000000   181.000000     2.000000   999.000000     0.000000   \n",
      "75%      47.000000   322.500000     3.000000   999.000000     0.000000   \n",
      "max      92.000000  4199.000000    41.000000   999.000000     6.000000   \n",
      "\n",
      "       emp.var.rate  cons.price.idx  cons.conf.idx    euribor3m  nr.employed  \n",
      "count   4119.000000     4119.000000    4119.000000  4119.000000  4119.000000  \n",
      "mean       0.095606       93.576445     -40.399806     3.640218  5167.591721  \n",
      "std        1.565630        0.580601       4.635554     1.728035    72.529141  \n",
      "min       -3.400000       92.201000     -50.800000     0.635000  4963.600000  \n",
      "25%       -1.800000       93.075000     -42.700000     1.354000  5099.100000  \n",
      "50%        1.100000       93.444000     -41.800000     4.857000  5191.000000  \n",
      "75%        1.400000       93.994000     -36.400000     4.961000  5228.100000  \n",
      "max        1.400000       94.767000     -26.900000     5.045000  5228.100000  \n",
      "Valores únicos en la columna job:\n",
      "['services' 'technician' 'entrepreneur' 'admin.' 'blue-collar' 'housemaid'\n",
      " 'management' 'student' 'unemployed' 'retired' 'self-employed' 'unknown']\n",
      "\n",
      "Valores únicos en la columna marital:\n",
      "['single' 'married' 'divorced' 'unknown']\n",
      "\n",
      "Valores únicos en la columna education:\n",
      "['high.school' 'professional.course' 'university.degree' 'unknown'\n",
      " 'basic.6y' 'basic.4y' 'basic.9y' 'illiterate']\n",
      "\n",
      "Valores únicos en la columna housing:\n",
      "['yes' 'no' 'unknown']\n",
      "\n",
      "Valores únicos en la columna loan:\n",
      "['no' 'yes' 'unknown']\n",
      "\n",
      "Valores únicos en la columna contact:\n",
      "['telephone' 'cellular']\n",
      "\n",
      "Valores únicos en la columna month:\n",
      "['may' 'jun' 'jul' 'aug' 'oct' 'nov' 'mar' 'apr' 'sep' 'dec']\n",
      "\n",
      "Valores únicos en la columna poutcome:\n",
      "['nonexistent' 'failure' 'success']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "##Preprocesamiento\n",
    "\n",
    "# Leer la base de datos\n",
    "url = \"https://raw.githubusercontent.com/jclavijo9612/proyectos/main/bank-additional-full.csv\"\n",
    "data = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "\n",
    "# Definir el porcentaje de registros a eliminar\n",
    "percent_to_drop = 0.9\n",
    "\n",
    "# Calcular el número de registros a eliminar\n",
    "num_records = len(data)\n",
    "num_records_to_drop = int(num_records * percent_to_drop)\n",
    "\n",
    "# Eliminar aleatoriamente el 90% de los registros\n",
    "data_sampled = data.sample(n=num_records_to_drop, random_state=42)\n",
    "\n",
    "# Mantener el 10% restante de los registros\n",
    "data_remaining = data.drop(data_sampled.index)\n",
    "\n",
    "# Verificar el tamaño del dataset resultante\n",
    "print(\"Tamaño del dataset original:\", num_records)\n",
    "print(\"Tamaño del dataset remanente:\", len(data_remaining))\n",
    "\n",
    "data=data_remaining\n",
    "\n",
    "# Visualización de los primeros registros\n",
    "print(data.head())\n",
    "\n",
    "# Resumen estadístico\n",
    "print(data.describe())\n",
    "\n",
    "# Variables categóricas\n",
    "categorical_cols = ['job', 'marital', 'education', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "# Iterar sobre las columnas categóricas y mostrar los valores únicos\n",
    "for column in categorical_cols:\n",
    "    unique_values = data[column].unique()\n",
    "    print(f\"Valores únicos en la columna {column}:\")\n",
    "    print(unique_values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa7bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazamos y para que sea 0 y 1\n",
    "data['y'] = data['y'].replace({'no': 0, 'yes': 1})\n",
    "\n",
    "\n",
    "#Quitar columna duration\n",
    "data = data.drop(['duration','day_of_week','default','pdays','campaign'], axis=1 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee583e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  job  marital  education  housing  loan  contact  month  previous  \\\n",
      "9    25    7        2          3        2     0        1      6         0   \n",
      "55   55    9        1          5        2     0        1      6         0   \n",
      "60   47    2        1          6        0     0        1      6         0   \n",
      "77   33    7        1          3        2     0        1      6         0   \n",
      "82   38    0        1          3        0     0        1      6         0   \n",
      "\n",
      "    poutcome  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  \\\n",
      "9          1           1.1          93.994          -36.4      4.857   \n",
      "55         1           1.1          93.994          -36.4      4.857   \n",
      "60         1           1.1          93.994          -36.4      4.857   \n",
      "77         1           1.1          93.994          -36.4      4.857   \n",
      "82         1           1.1          93.994          -36.4      4.857   \n",
      "\n",
      "    nr.employed  y  \n",
      "9        5191.0  0  \n",
      "55       5191.0  0  \n",
      "60       5191.0  0  \n",
      "77       5191.0  0  \n",
      "82       5191.0  0  \n"
     ]
    }
   ],
   "source": [
    "## Label Encoding de las variables categorica\n",
    "\n",
    "# Crear una instancia de LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Definir el orden para la columna 'month'\n",
    "month_order = ['mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "# Definir el orden para la columna 'day_of_week'\n",
    "day_of_week_order = ['mon', 'tue', 'wed', 'thu', 'fri']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col == 'month':\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(month_order)\n",
    "        data[col] = label_encoder.transform(data[col])\n",
    "    else:\n",
    "        data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# Mostrar los primeros registros del conjunto de datos con las variables categóricas codificadas\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3030b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imputación de nulls\n",
    "\n",
    "# Crea una instancia de SimpleImputer con la estrategia de imputación deseada (por ejemplo, media)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Imputa los valores nulos en el dataset\n",
    "data_imputed = imputer.fit_transform(data)\n",
    "\n",
    "# Convierte el resultado nuevamente en un DataFrame\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data.columns)\n",
    "\n",
    "data = data_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d71f6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Normalización de variables numéricas\n",
    "\n",
    "# Crea una instancia del escalador MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Selecciona las columnas numéricas a normalizar\n",
    "numeric_columns = ['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "# Aplica la normalización min-max a las columnas numéricas\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffcf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en el GridSearch\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# Crear el modelo de regresión logística\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "# Realizar GridSearchCV para encontrar los mejores hiperparámetros\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_model = LogisticRegression(solver='liblinear', **best_params)\n",
    "\n",
    "# Realizar validación cruzada y calcular la puntuación promedio\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "mean_cv_score = cv_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error (precisión)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Resultados del modelo de regresión lineal:\")\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "print(\"Puntuación promedio de validación cruzada:\", mean_cv_score)\n",
    "print(\"Precisión en el conjunto de prueba:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en el GridSearch\n",
    "param_grid = {\n",
    "    'penalty': ['l1'],\n",
    "    'C': [ 1]\n",
    "}\n",
    "\n",
    "# Crear el modelo de regresión logística\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "# Realizar GridSearchCV para encontrar los mejores hiperparámetros\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_model = LogisticRegression(solver='liblinear', **best_params)\n",
    "\n",
    "# Realizar validación cruzada y calcular la puntuación promedio\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "mean_cv_score = cv_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error (precisión)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Resultados del modelo de regresión lineal:\")\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "print(\"Puntuación promedio de validación cruzada:\", mean_cv_score)\n",
    "print(\"Precisión en el conjunto de prueba:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en GridSearch\n",
    "parameters = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [10],\n",
    "    'min_samples_split': [ 10],\n",
    "    'min_samples_leaf': [1],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Crear un modelo de árbol de decisión con validación cruzada\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "# Realizar validación cruzada y calcular el puntaje promedio\n",
    "cross_val_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "avg_score = cross_val_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de pruebas\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error y mostrar los resultados\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "print(\"Puntaje promedio de validación cruzada:\", avg_score)\n",
    "print(\"Exactitud en el conjunto de pruebas:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en GridSearch\n",
    "parameters = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_samples': [0.5, 0.8, 1.0],\n",
    "    'max_features': [0.5, 0.8, 1.0],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Crear un modelo de Bagging con validación cruzada\n",
    "grid_search = GridSearchCV(BaggingClassifier(), parameters, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_model = BaggingClassifier(**best_params)\n",
    "\n",
    "# Realizar validación cruzada y calcular el puntaje promedio\n",
    "cross_val_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "avg_score = cross_val_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de pruebas\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error y mostrar los resultados\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "print(\"Puntaje promedio de validación cruzada:\", avg_score)\n",
    "print(\"Exactitud en el conjunto de pruebas:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en GridSearch\n",
    "parameters = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Crear un modelo de RandomForest con validación cruzada\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), parameters, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_model = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Realizar validación cruzada y calcular el puntaje promedio\n",
    "cross_val_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "avg_score = cross_val_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de pruebas\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error y mostrar los resultados\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "print(\"Puntaje promedio de validación cruzada:\", avg_score)\n",
    "print(\"Exactitud en el conjunto de pruebas:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en GridSearch\n",
    "parameters = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Crear un modelo de GradientBoosting con validación cruzada\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(), parameters, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_model = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "# Realizar validación cruzada y calcular el puntaje promedio\n",
    "cross_val_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "avg_score = cross_val_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de pruebas\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error y mostrar los resultados\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "print(\"Puntaje promedio de validación cruzada:\", avg_score)\n",
    "print(\"Exactitud en el conjunto de pruebas:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en GridSearch\n",
    "parameters = {\n",
    "    'n_estimators': [100],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [3],\n",
    "    'subsample': [0.8],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Crear un modelo de GradientBoosting con validación cruzada\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(), parameters, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_model = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "# Realizar validación cruzada y calcular el puntaje promedio\n",
    "cross_val_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "avg_score = cross_val_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de pruebas\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error y mostrar los resultados\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "print(\"Puntaje promedio de validación cruzada:\", avg_score)\n",
    "print(\"Exactitud en el conjunto de pruebas:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26067e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e06805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en GridSearchCV\n",
    "params = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "# Crear un modelo de XGBoost\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Realizar GridSearchCV\n",
    "grid_search = GridSearchCV(xgb_model, params, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_model = xgb.XGBClassifier(**best_params)\n",
    "\n",
    "# Realizar validación cruzada y calcular el puntaje promedio\n",
    "scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "mean_score = np.mean(scores)\n",
    "\n",
    "# Ajustar el modelo final con todos los datos\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error (por ejemplo, precisión)\n",
    "error = 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados, los mejores hiperparámetros y el puntaje obtenido\n",
    "print(\"Resultados del modelo XGBoost:\")\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "print(\"Puntaje promedio de validación cruzada:\", mean_score)\n",
    "print(\"Error en el conjunto de prueba:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f206bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en GridSearchCV\n",
    "params = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Crear un modelo de XGBoost\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Realizar GridSearchCV\n",
    "grid_search = GridSearchCV(xgb_model, params, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_model = xgb.XGBClassifier(**best_params)\n",
    "\n",
    "# Realizar validación cruzada y calcular el puntaje promedio\n",
    "scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "mean_score = np.mean(scores)\n",
    "\n",
    "# Ajustar el modelo final con todos los datos\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error (por ejemplo, precisión)\n",
    "error = 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados, los mejores hiperparámetros y el puntaje obtenido\n",
    "print(\"Resultados del modelo XGBoost:\")\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "print(\"Puntaje promedio de validación cruzada:\", mean_score)\n",
    "print(\"Error en el conjunto de prueba:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c05f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['campaign', 'previous'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b326a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 100, 'solver': 'sgd'}\n",
      "Puntuación promedio de validación cruzada: 0.8813353566009103\n",
      "Precisión en el conjunto de prueba: 0.9065533980582524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en GridSearchCV\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(100)],\n",
    "    'activation': ['tanh'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [0.0001],\n",
    "}\n",
    "\n",
    "# Crear el modelo de Multi-Layer Perceptron\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Realizar GridSearchCV para encontrar los mejores hiperparámetros\n",
    "grid_search = GridSearchCV(mlp, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_mlp = MLPClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Realizar validación cruzada para obtener la puntuación promedio\n",
    "cv_scores = cross_val_score(best_mlp, X_train, y_train, cv=5)\n",
    "mean_cv_score = cv_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_mlp.predict(X_test)\n",
    "\n",
    "# Calcular la precisión del modelo final en el conjunto de prueba\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "print(\"Puntuación promedio de validación cruzada:\", mean_cv_score)\n",
    "print(\"Precisión en el conjunto de prueba:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3193b43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 100, 'solver': 'sgd'}\n",
      "Puntuación promedio de validación cruzada: 0.7319507849411848\n",
      "Precisión en el conjunto de prueba: 0.7235213204951857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en GridSearchCV\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(100)],\n",
    "    'activation': [ 'tanh'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [0.0001],\n",
    "}\n",
    "\n",
    "# Crear el modelo de Multi-Layer Perceptron\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Realizar GridSearchCV para encontrar los mejores hiperparámetros\n",
    "grid_search = GridSearchCV(mlp, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_mlp = MLPClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Realizar validación cruzada para obtener la puntuación promedio\n",
    "cv_scores = cross_val_score(best_mlp, X_train, y_train, cv=5)\n",
    "mean_cv_score = cv_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_mlp.predict(X_test)\n",
    "\n",
    "# Calcular la precisión del modelo final en el conjunto de prueba\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "print(\"Puntuación promedio de validación cruzada:\", mean_cv_score)\n",
    "print(\"Precisión en el conjunto de prueba:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84d73791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Puntaje de validación cruzada: 0.7190507426876221\n",
      "Error en el conjunto de test: 0.39958734525447037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Dividir el dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar en el GridSearch\n",
    "param_grid = {\n",
    "    'C': [0.1],\n",
    "    'kernel': ['linear'],\n",
    "    'gamma': ['scale']\n",
    "}\n",
    "\n",
    "# Crear el modelo SVC\n",
    "svc = SVC()\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros mediante validación cruzada\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "best_svc = SVC(**best_params)\n",
    "\n",
    "# Realizar validación cruzada y obtener el puntaje promedio\n",
    "cv_scores = cross_val_score(best_svc, X_train, y_train, cv=5)\n",
    "cv_score_mean = cv_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos\n",
    "best_svc.fit(X, y)\n",
    "\n",
    "# Realizar predicciones en el conjunto de test\n",
    "y_pred = best_svc.predict(X_test)\n",
    "\n",
    "# Calcular el error\n",
    "error = 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "print(\"Puntaje de validación cruzada:\", cv_score_mean)\n",
    "print(\"Error en el conjunto de test:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dde12ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 10}\n",
      "Puntaje promedio de validación cruzada: 0.9203902988183776\n",
      "Error en el conjunto de prueba: 0.08459422283356255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el clasificador base (Gradient Boosting)\n",
    "base_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Definir los parámetros a ajustar en el gridsearch\n",
    "param_grid = {\n",
    "    'n_estimators': [10],  # Número de estimadores en el ensamblado\n",
    "    'max_samples': [0.5],  # Tamaño de la muestra para cada estimador\n",
    "    'max_features': [0.5, 0.8],  # Número de características para cada estimador\n",
    "}\n",
    "\n",
    "# Crear el modelo de Bagging con el clasificador base\n",
    "bagging_model = BaggingClassifier(base_estimator=base_classifier)\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros mediante validación cruzada\n",
    "grid_search = GridSearchCV(bagging_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros y crear un nuevo modelo con ellos\n",
    "best_params = grid_search.best_params_\n",
    "best_model = BaggingClassifier(base_estimator=base_classifier, **best_params)\n",
    "\n",
    "# Realizar validación cruzada para obtener el puntaje promedio\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "average_score = cv_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error (por ejemplo, precisión o exactitud)\n",
    "error = 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "print(\"Puntaje promedio de validación cruzada:\", average_score)\n",
    "print(\"Error en el conjunto de prueba:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d3be48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 10.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 488, in fit\n",
      "    return super().fit(X, self._le.transform(y), sample_weight)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 158, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1088, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 42, in _fit_single_estimator\n",
      "    estimator.fit(X, y)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 255, in fit\n",
      "    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n",
      "  File \"C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 315, in _dense_fit\n",
      "    ) = libsvm.fit(\n",
      "  File \"sklearn\\svm\\_libsvm.pyx\", line 189, in sklearn.svm._libsvm.fit\n",
      "ValueError: C <= 0\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.88952959]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Clju2001\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'gradient_boosting__n_estimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1\\ipykernel_13516\\1136788574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# Obtener los mejores hiperparámetros y crear un nuevo modelo con ellos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Realizar validación cruzada para obtener el puntaje promedio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'gradient_boosting__n_estimators'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir los clasificadores base\n",
    "logistic_regression = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "bagging = BaggingClassifier()\n",
    "gradient_boosting = GradientBoostingClassifier()\n",
    "xgboost = XGBClassifier()\n",
    "svc = SVC()\n",
    "\n",
    "# Definir el ensamblador de Stacking\n",
    "estimators = [\n",
    "    ('logistic_regression', logistic_regression),\n",
    "    ('random_forest', random_forest),\n",
    "    ('bagging', bagging),\n",
    "    ('gradient_boosting', gradient_boosting),\n",
    "    ('xgboost', xgboost),\n",
    "    ('svc', svc)\n",
    "]\n",
    "stacking_model = StackingClassifier(estimators=estimators)\n",
    "\n",
    "# Definir los parámetros a ajustar en el gridsearch\n",
    "param_grid = {\n",
    "    'logistic_regression__C': [1],\n",
    "    'random_forest__n_estimators': [200],\n",
    "    'gradient_boosting__n_estimators': [100],\n",
    "    'xgboost__n_estimators': [100],\n",
    "    'svc__C': [0,1]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros mediante validación cruzada\n",
    "grid_search = GridSearchCV(stacking_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros y crear un nuevo modelo con ellos\n",
    "best_params = grid_search.best_params_\n",
    "best_model = StackingClassifier(estimators=estimators, **best_params)\n",
    "\n",
    "# Realizar validación cruzada para obtener el puntaje promedio\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "average_score = cv_scores.mean()\n",
    "\n",
    "# Ajustar el modelo final con todos los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular el error (por ejemplo, precisión o exactitud)\n",
    "error = 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "print(\"Puntaje promedio de validación cruzada:\", average_score)\n",
    "print(\"Error en el conjunto de prueba:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045ddfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
